{"ast":null,"code":"export default {};","map":{"version":3,"names":[],"sources":["/Users/tanghaoyun/Documents/GitHub/WebSiteForLab/web-zhang/src/components/Research/ResearchInterests.vue"],"sourcesContent":["<template>\n    <div class=\"research-content\">\n        <ul>\n            <li class=\"research_sentence\">\n                &nbsp;&nbsp;&nbsp;&nbsp;The Zhang has a long history of research on heuristic search, planning and optimization. The work has made significant contributions to the understanding of the nature and complexity of many search techniques, such as best-first search and depth-first branch-and-bound, and combinatorial optimization problems, such as the Traveling Salesman Problem and Constraint Satisfaction.\n            </li>\n            <li class=\"research_title\">Asymptotic optimality of linear-space search algorithms: </li>\n            <li class=\"research_sentence\">\n                &nbsp;&nbsp;&nbsp;&nbsp;THeuristic search is one of the cornerstone of AI. The best-first search (BFS) algorithm, with the well-known A* algorithm as a special case, is time optimal. However, it requires space that is exponential in the search depth of the underlying search tree, which is impractical for large combinatorial optimization problems, such as the Traveling Salesman Problem (TSP).\n                In contrast, a linear-space search (LSS) algorithm, with depth-first search and depth-first branch-and-bound (DFBnB) as special cases, runs linearly in search depth with a penalty of exploring more nodes in the search space. Due to their limited space requirement, LSS algorithms, including DFBnB, iterative deepening A* (IDA*), and recursive best-first search (RBFS), are the most popular techniques \n                for large and difficult search problems in practice. Understanding the expected complexity of LSS algorithms was an important and challenging problem.Based on a random search tree model proposed by Judea Pearl and Richard Karp,<b>our research established the <u>asymptotic optimality</u> of LSS methods</b>, showing that their expected complexity is a constant factor of that of BFS (<a class=\"research_link\" href=\"https://www.cse.wustl.edu/~weixiongzhang/\" target=\"_blank\">Zhand and Korf, Artificial Intelligence, \n                79(2):241-92, 1995</a>; <a class=\"research_link\" href=\"https://link.springer.com/book/10.1007/978-1-4612-1538-7\" target=\"_blank\">Zhang, State-Space Search, Springer, 1999</a>). Moreover, LSS algorithms may run faster than BFS on large problems, making them the algorithms for real applications.\n                <br/>\n                &nbsp;&nbsp;&nbsp;&nbsp;A directly and early application of our complexity results is the explanation to anomaly of the fixed horizon search, which was observed initially by Richard Korf on sliding-tile puzzles using DFBnB, as illustrated below. The anomaly states that for a fixed search depth, DFBnB can search the trees with larger branching factors faster. In fact, the anomaly persists regardless which search method, e.g., ID (or IDA*) or RBFS, shown below.\n                <li>\n                <img class=\"research_img_1\" src=\"\thttps://genomicmedicine.github.io/site/picture/0/dd7eea7dbc684d22aa404423aca5e170.png\">\n                <img class=\"research_img_2\" src=\"https://genomicmedicine.github.io/site/picture/0/24b0a4109cf841159f9678ca1c488c97.png\">\n                </li>\n            </li>\n            <li class=\"research_sentence\">\n                &nbsp;&nbsp;&nbsp;&nbsp;The insight gained by the complexity analysis on this search anomaly helped predict the existence of a phase transition in heuristic search problems, discussed below.\n            </li>\n\n\n            <li class=\"research_title\">Phase transitions and structures of combinatorial search: </li>\n            <li class=\"research_sentence\">\n                &nbsp;&nbsp;&nbsp;&nbsp;Our study also revealed that the expected complexity of BFS and LLS algorithms exhibits a sharp and abrupt transition from polynomial to exponential time as some critical problem features \n                (e.g., the precision of heuristic function) shift (<a class=\"research_link\" href=\"https://www.cse.wustl.edu/~weixiongzhang/\" target=\"_blank\">Zhand and Korf, Artificial Intelligence, 79(2):241-92, 1995</a>;<a class=\"research_link\" href=\"https://www.cse.wustl.edu/~weixiongzhang/\" target=\"_blank\">Zhang and Korf, Artificial Intelligence, 81(1-2):223-39, 1996</a> ; \n                <a class=\"research_link\" href=\"https://link.springer.com/book/10.1007/978-1-4612-1538-7\" target=\"_blank\">Zhang, State-Space Search, Springer, 1999</a>). This is similar to phase transitions of spin glasses studied in physics. A simple example of a phase transition is water changing from liquid to solid ice \n                when the temperature drops below the freezing point. We identified that the accuracy of heuristic information is the controlling parameter of phase transitions. On the Judea&Karp incremental random \n                search tree model, the phase transition can be summarized by the following figure.\n                <li>\n                <img class=\"research_img_3\" src=\"\thttps://genomicmedicine.github.io/site/picture/0/2eac6d39e18b4de4b02f846110bb9b8e.png\">\n                </li>\n            </li>\n\n            <li class=\"research_sentence\">\n                &nbsp;&nbsp;&nbsp;&nbsp;The control parameter of this phase transition is the product of the mean branching factor of the search tree and the probability if an edge cost has cost 0.\n                <br/>\n                &nbsp;&nbsp;&nbsp;&nbsp;This analysis was extended to reveal phase transitions in combinatorial optimization problems, such as the asymmetric TSP (ATSP) (<a class=\"research_link\" href=\"https://www.sciencedirect.com/science/article/pii/0004370295000542\" target=\"_blank\">Zhang and Korf, Artificial Intelligence, 81(1-2):223-39, 1996</a>) and Boolean Satisfiability \n                (<a class=\"research_link\" href=\"https://www.sciencedirect.com/science/article/pii/S0004370204000542\" target=\"_blank\">Zhang, Artificial Intelligence, 158:1-26, 2004</a>). For the ATSP, the control parameter is the precision of the distances between cities, which is equivalent to the number of digits used to represent distances. Intuitively, when the number of digits \n                used is small, many distances may have a high chance to have the same value, and consequently many complete tours (visiting each and every city exactly once) may have a high chance to have the same cost as well. This also means that a large number of optimal TSP tours exist;\n                 and finding one optimal TSP tour should be relatively easy. In contrast, when no two distances between cites are the same, the number of optimal TSP tour is small, so that finding an optimal solution is difficult. This phase transition is illustrated in the figure below, \n                 where the Assignment Problem (AP) is the main computation step (heuristic) for solving the ATSP.\n                 <li>\n                <img class=\"research_img_3\" src=\"\thttps://genomicmedicine.github.io/site/picture/0/d808eb0cb7c74017aae8fd95733deb5a.png\">\n                </li>\n            </li>\n\n            <li class=\"research_sentence\">\n                &nbsp;&nbsp;&nbsp;&nbsp;Interestingly, there also exist phase transitions in maximum Boolean satisfiability (max SAT), which is the optimization version of the SAT where the objective is to find a variable assignment that minimize the number of violated clauses (<a class=\"research_link\" href=\"https://www.sciencedirect.com/science/article/pii/S0004370204000542\" target=\"_blank\">Zhang, Artificial Intelligence, 158:1-26, 2004</a>).\n                <br/>\n                <b>&nbsp;&nbsp;&nbsp;&nbsp;Note that the phase transitions that we studied in optimization problems, such as finding the minimal TSP tour and the variable assignment to minimize the number of unsatisfied constraints, are fundamentally different from the phase transitions for decision problems, such as the decision version of the TSP and the SAT. For optimization problems, the phase transitions appear as an easy-to-difficult transition, whereas for decision problems, the phase transitions exhibit an easy-to-difficult-then-to-easy pattern.</b>\n                <br/>\n                &nbsp;&nbsp;&nbsp;&nbsp;The computational complexity and phase transition are also related to the backbone of a problem, which is a set of variables that have the same values among all solutions (Zhang, Proc. Intern. Conf. on Principles and Practice of Constraint programming, CP-2001; Zhang, JAIR, 20:471-97, 2004; Zhang, Artificial Intelligence, 158:1-26, 2004).\n            </li>\n        </ul>    \n    </div>\n </template>\n\n <script>\n    export default {\n\n    }\n</script>\n\n<style lang=\"less\" scoped>\n\n.research_img_1{\n   margin-left: 10%;\n   width: 230px;\n   padding-bottom: 20px;\n}\n\n.research_img_2{\n   width: 460px;\n   padding-bottom: 20px;\n}\n.research_img_3{\n   margin-left: 10%;\n   width: 400px;\n   padding-bottom: 20px;\n}\n\n</style>"],"mappings":"AA6DI,eAAe,CAEf"},"metadata":{},"sourceType":"module","externalDependencies":[]}